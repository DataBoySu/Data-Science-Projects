{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72275c84",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4bc0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Python3106\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from transformers_interpret import SequenceClassificationExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c21052",
   "metadata": {},
   "source": [
    "## Step 1: Load your model + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71c0b5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loca = \"./model/best_model_f\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(loca)\n",
    "tokenizer = AutoTokenizer.from_pretrained(loca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea36a58",
   "metadata": {},
   "source": [
    "## Step 2: Load training data (minority examples only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640fcf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/minority_train.csv\")\n",
    "df = df[[\"text\", \"label\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49138702",
   "metadata": {},
   "source": [
    "## Step 2.5: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65c570c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_data(\n",
    "    df,\n",
    "    text_column=\"text\",\n",
    "    min_words=3,\n",
    "    expand_contractions=True,\n",
    "    clean_text_flag=True,\n",
    "    clean_garbage_flag=True,\n",
    "    delete_short_flag=True,\n",
    "    stratified_shuffle=True,   # NEW: enable/disable stratified shuffle\n",
    "    label_column=\"label\",\n",
    "    n_splits=6,\n",
    "    random_state=421\n",
    "):\n",
    "    contractions_dict = {\n",
    "        \"can't\": \"cannot\", \"won't\": \"will not\", \"i'm\": \"i am\", \"it's\": \"it is\", \"he's\": \"he is\",\n",
    "        \"she's\": \"she is\", \"they're\": \"they are\", \"we're\": \"we are\", \"you're\": \"you are\",\n",
    "        \"i've\": \"i have\", \"don't\": \"do not\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
    "        \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "        \"hasn't\": \"has not\", \"haven't\": \"have not\", \"hadn't\": \"had not\", \"shouldn't\": \"should not\",\n",
    "        \"wouldn't\": \"would not\", \"couldn't\": \"could not\", \"mustn't\": \"must not\", \"let's\": \"let us\",\n",
    "        \"that's\": \"that is\", \"what's\": \"what is\", \"there's\": \"there is\", \"who's\": \"who is\",\n",
    "        \"where's\": \"where is\", \"how's\": \"how is\", \"here's\": \"here is\", \"i'll\": \"i will\",\n",
    "        \"you'll\": \"you will\", \"he'll\": \"he will\", \"she'll\": \"she will\", \"we'll\": \"we will\",\n",
    "        \"they'll\": \"they will\", \"i'd\": \"i would\", \"you'd\": \"you would\", \"he'd\": \"he would\",\n",
    "        \"she'd\": \"she would\", \"we'd\": \"we would\", \"they'd\": \"they would\", \"n't\": \" not\",\n",
    "        \"'re\": \" are\", \"'s\": \" is\", \"'d\": \" would\", \"'ll\": \" will\", \"'ve\": \" have\", \"'m\": \" am\"\n",
    "    }\n",
    "    contractions_re = re.compile('(%s)' % '|'.join(map(re.escape, contractions_dict.keys())))\n",
    "\n",
    "    def expand(text):\n",
    "        return contractions_re.sub(lambda m: contractions_dict[m.group(0)], text)\n",
    "\n",
    "    def clean(text):\n",
    "        if expand_contractions:\n",
    "            text = expand(text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"\\b(?:href|http|https|www)\\b|\\b(?:href|http|https|www)\\S+\", \"\", text)\n",
    "        text = re.sub(r\"@\\w+\", \"\", text)\n",
    "        text = re.sub(r\"\\s+#\", \" #\", text)\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "        punct_to_remove = string.punctuation.replace(\"!\", \"\").replace(\"?\", \"\").replace(\"'\", \"\")\n",
    "        text = text.translate(str.maketrans(\"\", \"\", punct_to_remove))\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "\n",
    "    if clean_text_flag:\n",
    "        df[text_column] = df[text_column].astype(str).apply(clean)\n",
    "\n",
    "    if clean_garbage_flag:\n",
    "        df = df.dropna()\n",
    "        df[text_column + '_norm'] = df[text_column].str.lower().str.strip()\n",
    "        df = df.drop_duplicates(subset=[text_column + '_norm'])\n",
    "        df = df.drop(columns=[text_column + '_norm'])\n",
    "        df = df[df[text_column].str.strip().astype(bool)]\n",
    "\n",
    "    if delete_short_flag:\n",
    "        short_rows = df[df[text_column].apply(lambda x: len(str(x).split()) < min_words)]\n",
    "        if not short_rows.empty:\n",
    "            print(f\"Short sentences (less than {min_words} words):\")\n",
    "            print(short_rows[[text_column]] if label_column not in df.columns else short_rows[[text_column, label_column]])\n",
    "            confirm = input(f\"\\nDelete these {len(short_rows)} rows? (yes/no): \").strip().lower()\n",
    "            if confirm == \"yes\":\n",
    "                df = df.drop(short_rows.index).reset_index(drop=True)\n",
    "                print(f\"Deleted {len(short_rows)} short sentences.\")\n",
    "            else:\n",
    "                print(\"No rows deleted.\")\n",
    "\n",
    "    # --- Stratified Shuffle (from your code) ---\n",
    "    if stratified_shuffle:\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        import numpy as np\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        indices = []\n",
    "        for _, idx in skf.split(df[text_column], df[label_column]):\n",
    "            idx = np.array(idx)\n",
    "            np.random.shuffle(idx)  # Shuffle indices within each block\n",
    "            indices.extend(idx)\n",
    "        df = df.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bf75064",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_text_data(df, text_column=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb23717",
   "metadata": {},
   "source": [
    "## Step 2.6: Label Mapping and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835db139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792c9a5cd75042649821095884cde5ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/493 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_list = sorted(df['label'].unique())\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "\n",
    "df['label'] = df['label'].map(label2id)\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d4fba",
   "metadata": {},
   "source": [
    "## Step 3: LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b140b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 594,438 || all params: 185,021,196 || trainable%: 0.3213\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80711573",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Training args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b593273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SunAda\\AppData\\Local\\Temp\\ipykernel_27608\\299073688.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"lora_checkpoints/minority_patch\",\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    num_train_epochs=8,\n",
    "    learning_rate=3e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    label_smoothing_factor = 0.0,\n",
    "    label_names=[\"label\"],  # Add this line\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc5b49",
   "metadata": {},
   "source": [
    "## Step 5: Train!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7783346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='248' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [248/248 01:02, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.451500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.022300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.078400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.742900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.750600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.595900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.364600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.253900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.063600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.045800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=248, training_loss=1.5346246457869006, metrics={'train_runtime': 63.2858, 'train_samples_per_second': 62.32, 'train_steps_per_second': 3.919, 'total_flos': 38778112082880.0, 'train_loss': 1.5346246457869006, 'epoch': 8.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fba320",
   "metadata": {},
   "source": [
    "## Step 6: Save LoRA adapter only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2e069b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_checkpoints/minority_patch\\\\tokenizer_config.json',\n",
       " 'lora_checkpoints/minority_patch\\\\special_tokens_map.json',\n",
       " 'lora_checkpoints/minority_patch\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_checkpoints/minority_patch\")\n",
    "tokenizer.save_pretrained(\"lora_checkpoints/minority_patch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f9df1",
   "metadata": {},
   "source": [
    "## Step 7: Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "632ddcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,614 || all params: 185,021,196 || trainable%: 0.0025\n",
      "Active adapters: {'default': LoraConfig(task_type='SEQ_CLS', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='./model/best_model_f', revision=None, inference_mode=True, r=16, target_modules={'query_proj', 'value_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['classifier', 'score', 'classifier', 'score'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n"
     ]
    }
   ],
   "source": [
    "# Load base\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"model/best_model_f\")\n",
    "\n",
    "# Apply LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \n",
    "                                  \"lora_checkpoints/minority_patch\", \n",
    "                                  label2id=label2id,\n",
    "                                    id2label=id2label)\n",
    "model.print_trainable_parameters()\n",
    "# Sanity check: are LoRA adapters active?\n",
    "print(\"Active adapters:\", model.peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c060ae",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9208d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(texts):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "            probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "            label_id = torch.argmax(probs, dim=1).item()\n",
    "            confidence = probs[0][label_id].item()\n",
    "        predictions.append((text, label_id, confidence))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f771dda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He smiled as he read the unexpected letter. → surprise (conf: 0.56)\n",
      "Her hands shook as she heard the strange noise. → surprise (conf: 0.53)\n",
      "He opened the door and gasped. → fear (conf: 0.52)\n",
      "The envelope contained a ticket to a place I had never heard of → joy (conf: 0.46)\n"
     ]
    }
   ],
   "source": [
    "minority_test_samples = [\n",
    "    \"He smiled as he read the unexpected letter.\",        # joy\n",
    "    \"Her hands shook as she heard the strange noise.\",    # fear\n",
    "    \"He opened the door and gasped.\",                          # surprise     \n",
    "    \"The envelope contained a ticket to a place I had never heard of\",# surprise\n",
    "    \n",
    "]\n",
    "\n",
    "results = predict_emotion(minority_test_samples)\n",
    "for text, label_id, conf in results:\n",
    "    label_id = id2label[label_id]\n",
    "    print(f\"{text} → {label_id} (conf: {conf:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e5b1b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>fear (0.53)</b></text></td><td><text style=\"padding-right:2em\"><b>fear</b></text></td><td><text style=\"padding-right:2em\"><b>-0.22</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁He                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁opened                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁door                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁gasped                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>fear (0.53)</b></text></td><td><text style=\"padding-right:2em\"><b>fear</b></text></td><td><text style=\"padding-right:2em\"><b>-0.22</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁He                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁opened                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁door                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 67%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁gasped                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer = SequenceClassificationExplainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "word_attributions = explainer(\"He opened the door and gasped.\")\n",
    "explainer.visualize()  # or print(word_attributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bdd805",
   "metadata": {},
   "source": [
    "# Continue Training the Lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ecfcb1",
   "metadata": {},
   "source": [
    "## Step 9: Train Lora further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edaddd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV and convert to HuggingFace Dataset\n",
    "df = pd.read_csv(\"./data/borderline_signal.csv\")\n",
    "\n",
    "# Optional: drop duplicates or near-duplicates\n",
    "df = preprocess_text_data(df, text_column=\"text\")\n",
    "\n",
    "# Map label strings to ids\n",
    "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df[[\"text\", \"label_id\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d78d79",
   "metadata": {},
   "source": [
    "## Step 10: Tokenize new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a83845e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cdb0b1310843868cfeff0fcfe66243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/175 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label_id\", \"label\")\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead6d733",
   "metadata": {},
   "source": [
    "## Step 11: Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e763fda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SunAda\\AppData\\Local\\Temp\\ipykernel_27608\\3823117642.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_checkpoints/minority_patch\",\n",
    "    per_device_train_batch_size=4,         # smaller batch → more granular LoRA updates\n",
    "    gradient_accumulation_steps=2,         # effective batch size = 8\n",
    "    num_train_epochs=6,                    # memorize-level training\n",
    "    learning_rate=5e-5,                    # much better for LoRA tuning\n",
    "    weight_decay=0.0,                      # no need — your examples are high-quality\n",
    "    warmup_ratio=0.1,                      # optional, helps smooth out first few steps\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=False,\n",
    "    remove_unused_columns=False,\n",
    "    overwrite_output_dir=True,\n",
    "    run_name=\"lora-phase2-borderline\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abba3a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 00:10, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.001400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.909000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.789700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.813500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.839400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.982300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.656100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.793600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.845000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.738500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.828200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.757400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.737700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=132, training_loss=1.8274004784497349, metrics={'train_runtime': 10.378, 'train_samples_per_second': 101.175, 'train_steps_per_second': 12.719, 'total_flos': 8150358510000.0, 'train_loss': 1.8274004784497349, 'epoch': 6.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f993b3",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfec39cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He smiled as he read the unexpected letter. → surprise (conf: 0.52)\n",
      "Her hands shook as she heard the strange noise. → surprise (conf: 0.64)\n",
      "He opened the door and gasped. → surprise (conf: 0.54)\n",
      "The letter made her tear up with joy. → joy (conf: 0.89)\n",
      "I felt a deep hole in my chest all day → sadness (conf: 0.51)\n",
      "He smiled as he read the unexpected letter. → surprise (conf: 0.52)\n",
      "Her hands shook as she heard the strange noise. → surprise (conf: 0.64)\n",
      "He opened the door and gasped. → surprise (conf: 0.54)\n",
      "The envelope contained a ticket to a place I had never heard of → joy (conf: 0.47)\n"
     ]
    }
   ],
   "source": [
    "minority_test_samples = [\n",
    "    \"He smiled as he read the unexpected letter.\",        # joy\n",
    "    \"Her hands shook as she heard the strange noise.\",    # fear\n",
    "    \"He opened the door and gasped.\",                              # surprise\n",
    "    \"The letter made her tear up with joy.\",                       # joy (control)\n",
    "    \"I felt a deep hole in my chest all day\",\n",
    "    \"He smiled as he read the unexpected letter.\",        # joy\n",
    "    \"Her hands shook as she heard the strange noise.\",    # fear\n",
    "    \"He opened the door and gasped.\",                          # surprise     \n",
    "    \"The envelope contained a ticket to a place I had never heard of\",# surprise\n",
    "]\n",
    "\n",
    "results = predict_emotion(minority_test_samples)\n",
    "for text, label_id, conf in results:\n",
    "    label_id = id2label[label_id]\n",
    "    print(f\"{text} → {label_id} (conf: {conf:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e85245f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>4</b></text></td><td><text style=\"padding-right:2em\"><b>sadness (0.51)</b></text></td><td><text style=\"padding-right:2em\"><b>sadness</b></text></td><td><text style=\"padding-right:2em\"><b>2.47</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁I                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁felt                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁deep                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁hole                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁my                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁chest                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁all                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁day                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>4</b></text></td><td><text style=\"padding-right:2em\"><b>sadness (0.51)</b></text></td><td><text style=\"padding-right:2em\"><b>sadness</b></text></td><td><text style=\"padding-right:2em\"><b>2.47</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁I                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁felt                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁deep                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁hole                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁my                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁chest                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁all                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁day                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "\n",
    "explainer = SequenceClassificationExplainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "word_attributions = explainer(\"I felt a deep hole in my chest all day\")\n",
    "explainer.visualize()  # or print(word_attributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab9fa0c",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ba69194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_checkpoints/minority_patch\\\\tokenizer_config.json',\n",
       " 'lora_checkpoints/minority_patch\\\\special_tokens_map.json',\n",
       " 'lora_checkpoints/minority_patch\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_checkpoints/minority_patch\")\n",
    "tokenizer.save_pretrained(\"lora_checkpoints/minority_patch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1515800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,614 || all params: 185,021,196 || trainable%: 0.0025\n",
      "Active adapters: {'default': LoraConfig(task_type='SEQ_CLS', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='./model/best_model_f', revision=None, inference_mode=True, r=16, target_modules={'query_proj', 'value_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['classifier', 'score', 'classifier', 'score', 'classifier', 'score'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"model/best_model_f\",\n",
    "                                                                label2id=label2id,\n",
    "                                                                id2label=id2label)\n",
    "# Apply LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \n",
    "                                  \"lora_checkpoints/minority_patch\", \n",
    "                                  label2id=label2id,\n",
    "                                    id2label=id2label)\n",
    "model.print_trainable_parameters()\n",
    "# Sanity check: are LoRA adapters active?\n",
    "print(\"Active adapters:\", model.peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9315ba61",
   "metadata": {},
   "source": [
    "# Continue Training Lora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2f58d0",
   "metadata": {},
   "source": [
    "## Step 12: Load and Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "277271f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/logic_inversion.csv\")\n",
    "df = preprocess_text_data(df, text_column=\"text\")\n",
    "\n",
    "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "dataset = Dataset.from_pandas(df[[\"text\", \"label_id\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d0221f",
   "metadata": {},
   "source": [
    "## Step 13: Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf001edf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0986383d19414294a7345e7033355ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label_id\", \"label\")\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b25a79",
   "metadata": {},
   "source": [
    "## Step 14: Training Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5528a7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SunAda\\AppData\\Local\\Temp\\ipykernel_27608\\1920385646.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_checkpoints/logic_patch\",\n",
    "    per_device_train_batch_size=4,         # smaller batch → more granular LoRA updates\n",
    "    gradient_accumulation_steps=2,         # effective batch size = 8\n",
    "    num_train_epochs=6,                    # memorize-level training\n",
    "    learning_rate=5e-5,                    # much better for LoRA tuning\n",
    "    weight_decay=0.0,                      # no need — your examples are high-quality\n",
    "    warmup_ratio=0.1,                      # optional, helps smooth out first few steps\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=False,\n",
    "    remove_unused_columns=False,\n",
    "    overwrite_output_dir=True,\n",
    "    run_name=\"lora-phase3-inversion\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96021b6",
   "metadata": {},
   "source": [
    "## Step 15: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3381a871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/234 00:17, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.499800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.317700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.489200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.334600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.585700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.344300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.567600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.304600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.581000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.458600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.286700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.502600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.406000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.308700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=234, training_loss=1.3985956102354913, metrics={'train_runtime': 17.2763, 'train_samples_per_second': 105.926, 'train_steps_per_second': 13.545, 'total_flos': 18939880728000.0, 'train_loss': 1.3985956102354913, 'epoch': 6.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14a66e",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "026133e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,614 || all params: 185,021,196 || trainable%: 0.0025\n",
      "Active adapters: {'default': LoraConfig(task_type='SEQ_CLS', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='./model/best_model_f', revision=None, inference_mode=True, r=16, target_modules={'query_proj', 'value_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['classifier', 'score', 'classifier', 'score', 'classifier', 'score', 'classifier', 'score'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n"
     ]
    }
   ],
   "source": [
    "## Step 6: Save LoRA adapter only\n",
    "model.save_pretrained(\"lora_checkpoints/logic_patch\")\n",
    "tokenizer.save_pretrained(\"lora_checkpoints/logic_patch\")\n",
    "# Load base\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"model/best_model_f\")\n",
    "\n",
    "# Apply LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \n",
    "                                  \"lora_checkpoints/logic_patch\", \n",
    "                                  label2id=label2id,\n",
    "                                    id2label=id2label)\n",
    "model.print_trainable_parameters()\n",
    "# Sanity check: are LoRA adapters active?\n",
    "print(\"Active adapters:\", model.peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae8a35cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I clenched my jaw as they congratulated the thief. → anger (conf: 0.63)\n",
      "\n",
      "He smiled, and I wanted to scream. → anger (conf: 0.78)\n",
      "\n",
      "Each word she said felt like another slap. → anger (conf: 0.79)\n",
      "\n",
      "I broke the plate—not by accident. → fear (conf: 0.47)\n",
      "\n",
      "They praised him again, and I walked out. → anger (conf: 0.79)\n",
      "\n",
      "My hands shook as the door creaked open. → fear (conf: 0.84)\n",
      "\n",
      "I pretended not to hear the footsteps behind me. → fear (conf: 0.86)\n",
      "\n",
      "The sudden silence in the woods felt too loud. → fear (conf: 0.50)\n",
      "\n",
      "Every shadow on the wall made my breath catch. → fear (conf: 0.80)\n",
      "\n",
      "He wasn’t supposed to be there, but he was. → fear (conf: 0.50)\n",
      "\n",
      "I couldn't stop laughing, even with tears in my eyes. → surprise (conf: 0.84)\n",
      "\n",
      "The music played and my heart danced. → surprise (conf: 0.81)\n",
      "\n",
      "He brought coffee, and it was exactly how I like it. → love (conf: 0.74)\n",
      "\n",
      "She said yes, and the world slowed down for a second. → surprise (conf: 0.83)\n",
      "\n",
      "I watched the sunrise, completely at peace. → joy (conf: 0.72)\n",
      "\n",
      "She always remembers the little things I forget. → sadness (conf: 0.28)\n",
      "\n",
      "Even in silence, we said everything. → sadness (conf: 0.65)\n",
      "\n",
      "He held my hand like it was the only thing that mattered. → joy (conf: 0.55)\n",
      "\n",
      "She stayed up just to hear how my day went. → surprise (conf: 0.79)\n",
      "\n",
      "I made dinner the way his mom used to. → love (conf: 0.96)\n",
      "\n",
      "The laughter felt distant, like it wasn't mine. → sadness (conf: 0.73)\n",
      "\n",
      "I stood in the same room, but everything had changed. → fear (conf: 0.47)\n",
      "\n",
      "Even his favorite song felt hollow today. → sadness (conf: 0.96)\n",
      "\n",
      "She left the light on, but no one came back. → fear (conf: 0.56)\n",
      "\n",
      "I wanted to smile, but it just wouldn’t come. → surprise (conf: 0.38)\n",
      "\n",
      "The box had my name, but I never told anyone. → sadness (conf: 0.24)\n",
      "\n",
      "She opened the door, and there he was. → fear (conf: 0.59)\n",
      "\n",
      "He remembered the anniversary I forgot. → anger (conf: 0.31)\n",
      "\n",
      "That call at midnight changed everything. → surprise (conf: 0.35)\n",
      "\n",
      "When I turned around, the crowd was cheering. → surprise (conf: 0.89)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_challenging_samples = [\n",
    "    #anger\n",
    "    \"I clenched my jaw as they congratulated the thief.\",\n",
    "    \"He smiled, and I wanted to scream.\",\n",
    "    \"Each word she said felt like another slap.\",\n",
    "    \"I broke the plate—not by accident.\",\n",
    "    \"They praised him again, and I walked out.\",\n",
    "    #fear\n",
    "    \"My hands shook as the door creaked open.\",\n",
    "    \"I pretended not to hear the footsteps behind me.\",\n",
    "    \"The sudden silence in the woods felt too loud.\",\n",
    "    \"Every shadow on the wall made my breath catch.\",\n",
    "    \"He wasn’t supposed to be there, but he was.\",\n",
    "    #joy\n",
    "    \"I couldn't stop laughing, even with tears in my eyes.\",\n",
    "    \"The music played and my heart danced.\",\n",
    "    \"He brought coffee, and it was exactly how I like it.\",\n",
    "    \"She said yes, and the world slowed down for a second.\",\n",
    "    \"I watched the sunrise, completely at peace.\",\n",
    "    #love\n",
    "    \"She always remembers the little things I forget.\",\n",
    "    \"Even in silence, we said everything.\",\n",
    "    \"He held my hand like it was the only thing that mattered.\",\n",
    "    \"She stayed up just to hear how my day went.\",\n",
    "    \"I made dinner the way his mom used to.\",\n",
    "    #sadness\n",
    "    \"The laughter felt distant, like it wasn't mine.\",\n",
    "    \"I stood in the same room, but everything had changed.\",\n",
    "    \"Even his favorite song felt hollow today.\",\n",
    "    \"She left the light on, but no one came back.\",\n",
    "    \"I wanted to smile, but it just wouldn’t come.\",\n",
    "    #surprise\n",
    "    \"The box had my name, but I never told anyone.\",\n",
    "    \"She opened the door, and there he was.\",\n",
    "    \"He remembered the anniversary I forgot.\",\n",
    "    \"That call at midnight changed everything.\",\n",
    "    \"When I turned around, the crowd was cheering.\",\n",
    "   \n",
    "]\n",
    "\n",
    "results = predict_emotion(eval_challenging_samples)\n",
    "for text, label_id, conf in results:\n",
    "    label_id = id2label[label_id]\n",
    "    print(f\"{text} → {label_id} (conf: {conf:.2f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040b47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>5</b></text></td><td><text style=\"padding-right:2em\"><b>surprise (0.35)</b></text></td><td><text style=\"padding-right:2em\"><b>surprise</b></text></td><td><text style=\"padding-right:2em\"><b>0.17</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁He                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁remembered                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁anniversary                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁I                    </font></mark><mark style=\"background-color: hsl(120, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁forgot                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>5</b></text></td><td><text style=\"padding-right:2em\"><b>surprise (0.35)</b></text></td><td><text style=\"padding-right:2em\"><b>surprise</b></text></td><td><text style=\"padding-right:2em\"><b>0.17</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁He                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁remembered                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁anniversary                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁I                    </font></mark><mark style=\"background-color: hsl(120, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁forgot                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ▁.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer = SequenceClassificationExplainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "word_attributions = explainer(\"He remembered the anniversary I forgot.\")\n",
    "explainer.visualize()  # or print(word_attributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca25a3a",
   "metadata": {},
   "source": [
    "# Continue Training LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5b4f7",
   "metadata": {},
   "source": [
    "## Step 16: Load and Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "145aacc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/final_finetune.csv\")\n",
    "df = preprocess_text_data(df, text_column=\"text\")\n",
    "\n",
    "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "dataset = Dataset.from_pandas(df[[\"text\", \"label_id\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8517f7e0",
   "metadata": {},
   "source": [
    "## Step 17: Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f0a9605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0219aa4347a44839a26f6d5b00b641ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label_id\", \"label\")\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbcad9a",
   "metadata": {},
   "source": [
    "## Step 18: Training Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fc5594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SunAda\\AppData\\Local\\Temp\\ipykernel_27608\\2295891324.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_checkpoints/final_patch\",\n",
    "    per_device_train_batch_size=4,         # smaller batch → more granular LoRA updates\n",
    "    gradient_accumulation_steps=2,         # effective batch size = 8\n",
    "    num_train_epochs=6,                    # memorize-level training\n",
    "    learning_rate=5e-5,                    # much better for LoRA tuning\n",
    "    weight_decay=0.0,                      # no need — your examples are high-quality\n",
    "    warmup_ratio=0.1,                      # optional, helps smooth out first few steps\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=False,\n",
    "    remove_unused_columns=False,\n",
    "    overwrite_output_dir=True,\n",
    "    run_name=\"lora-final\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4a9bf",
   "metadata": {},
   "source": [
    "## Step 18: Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf8b0303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 00:07, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.473000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.440300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.365800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.594500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.406400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.536600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.292200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.449200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=1.4100165168444316, metrics={'train_runtime': 7.5275, 'train_samples_per_second': 98.041, 'train_steps_per_second': 12.753, 'total_flos': 6492342721680.0, 'train_loss': 1.4100165168444316, 'epoch': 6.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a3b151",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2eca7617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,614 || all params: 185,021,196 || trainable%: 0.0025\n",
      "Active adapters: {'default': LoraConfig(task_type='SEQ_CLS', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='./model/best_model_f', revision=None, inference_mode=True, r=16, target_modules={'query_proj', 'value_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['classifier', 'score', 'classifier', 'score', 'classifier', 'score', 'classifier', 'score', 'classifier', 'score'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n"
     ]
    }
   ],
   "source": [
    "## Step 6: Save LoRA adapter only\n",
    "model.save_pretrained(\"lora_checkpoints/final_patch\")\n",
    "tokenizer.save_pretrained(\"lora_checkpoints/final_patch\")\n",
    "# Load base\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\"model/best_model_f\")\n",
    "\n",
    "# Apply LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \n",
    "                                  \"lora_checkpoints/final_patch\", \n",
    "                                  label2id=label2id,\n",
    "                                    id2label=id2label)\n",
    "model.print_trainable_parameters()\n",
    "# Sanity check: are LoRA adapters active?\n",
    "print(\"Active adapters:\", model.peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e676aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_sentences = [\n",
    "    \"She whispered 'I love you,' but it sounded like goodbye.\",        # love\n",
    "    \"I tried to smile at the joke, yet each word felt like a dagger.\", # anger\n",
    "    \"He stepped into the clearing expecting calm—and found a riot.\",   # surprise\n",
    "    \"I told myself it was okay, but my legs refused to move.\",        # fear\n",
    "    \"She danced around the room, but her tears fell like rain.\",      # sadness\n",
    "    \"He held the gift in trembling hands, though his heart leapt.\",   # joy\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9f67caf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She whispered 'I love you,' but it sounded like goodbye. → love (conf: 0.73)\n",
      "\n",
      "I tried to smile at the joke, yet each word felt like a dagger. → joy (conf: 0.29)\n",
      "\n",
      "He stepped into the clearing expecting calm—and found a riot. → surprise (conf: 0.52)\n",
      "\n",
      "I told myself it was okay, but my legs refused to move. → joy (conf: 0.55)\n",
      "\n",
      "She danced around the room, but her tears fell like rain. → surprise (conf: 0.54)\n",
      "\n",
      "He held the gift in trembling hands, though his heart leapt. → fear (conf: 0.81)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = predict_emotion(final_test_sentences)\n",
    "for text, label_id, conf in results:\n",
    "    label_id = id2label[label_id]\n",
    "    print(f\"{text} → {label_id} (conf: {conf:.2f})\")\n",
    "    print()\n",
    "explainer = SequenceClassificationExplainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa68d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_attributions = explainer(\"He remembered the anniversary I forgot.\")\n",
    "explainer.visualize()  # or print(word_attributions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
